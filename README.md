# ASRS Multi-Label Classification

**Master's Thesis** -- New Bulgarian University, 2026
**Title:** Multimodal Deep Learning for Runway Incursion Detection
**Author:** Zarko Rashev

## Research Question

Can large language models match or exceed classic machine learning on multi-label classification of aviation safety reports?

## Dataset

- **Source:** NASA Aviation Safety Reporting System (ASRS)
- **Total reports:** 172,183 narrative text reports
- **Parent labels:** 13 anomaly categories (multi-label)
- **Subcategory labels:** 48 fine-grained anomaly types (multi-label)
- **Train/test split:** 31,850 / 8,044 (stratified via `MultilabelStratifiedShuffleSplit`)

## Key Results (Parent Task, 13 Labels)

| Model | Approach | Macro-F1 | Micro-F1 | Macro-AUC | Cost |
|-------|----------|----------|----------|-----------|------|
| **XGBoost** | TF-IDF + Classic ML | **0.693** | **0.746** | **0.932** | $0 |
| DeepSeek V3.2 (671B) | Zero-shot + thinking | 0.681 | 0.723 | 0.810 | $6.73 |
| Mistral Large 3 (675B) | Zero-shot | 0.658 | 0.712 | 0.793 | $0 |
| Qwen3-8B | Few-shot + taxonomy | 0.526 | 0.544 | 0.706 | $0.45 |
| Qwen3-8B | QLoRA fine-tuned | 0.510 | 0.632 | 0.700 | $10.83 |

Classic ML (TF-IDF + XGBoost) is the overall winner. The best LLM (DeepSeek V3.2 671B + thinking) comes within 0.012 Macro-F1 but at significantly higher cost and latency.

## Models Evaluated

- **Classic ML:** XGBoost, LinearSVC, Logistic Regression (all with TF-IDF features)
- **Open-weight LLMs:** Qwen3-8B, Ministral 8B, Mistral Large 3 (675B MoE), DeepSeek V3.2 (671B MoE)
- **Approaches:** Zero-shot, few-shot, fine-tuned (QLoRA/LoRA), taxonomy-enriched prompts, thinking mode

## Repository Structure

```
notebooks/
  01_data_exploration.ipynb       # EDA, label distributions, co-occurrence
  02_experiment_pipeline.ipynb    # Stratified sampling, classic ML baseline
  03_final_visualizations.ipynb   # All figures for thesis
  05_classic_ml_subcategory.ipynb # 48-label classic ML analysis

scripts/
  # Classic ML
  modal_classic_ml_full.py        # XGBoost on full 164K dataset
  modal_classic_ml_subcategory.py # XGBoost on 48 subcategory labels
  modal_classic_ml_tuning.py      # Hyperparameter tuning (XGB, SVC, LogReg)
  modal_classic_ml_phase3.py      # Phase 3 tuned evaluation
  local_classic_ml_tuning.py      # Local tuning script

  # Qwen3-8B experiments
  modal_zero_shot.py              # Zero-shot inference
  modal_few_shot.py               # Few-shot inference
  modal_finetune.py               # QLoRA fine-tuning + inference
  modal_zero_shot_taxonomy.py     # Taxonomy-enriched zero-shot
  modal_few_shot_taxonomy.py      # Taxonomy-enriched few-shot
  modal_few_shot_taxonomy_thinking.py  # Thinking mode experiment
  modal_zero_shot_subcategory.py  # 48-label zero-shot

  # Mistral Large 3 experiments
  mistral_large_zero_shot.py      # Batch API zero-shot
  mistral_large_few_shot.py       # Batch API few-shot
  mistral_large_subcategory.py    # 48-label zero-shot

  # DeepSeek V3.2 experiments
  deepseek_v32_deepinfra.py       # Parent + subcategory, with/without thinking

  # Other
  glm5_zero_shot.py               # GLM-5 experiment (Modal)
  glm5_deepinfra.py               # GLM-5 experiment (DeepInfra)
  build_subcategory_dataset.py    # Build 48-label dataset from raw ASRS data

data/
  structured_features.csv         # Extracted structured features (tracked)
  # Train/test CSVs gitignored (regenerated by notebooks)

results/
  FINAL_EXPERIMENT_SUMMARY.md     # Complete results across all experiments
  # Per-experiment: *_metrics.csv, *_raw_outputs.csv, *_summary.txt
  # Visualizations: *.png
  ministral/                      # Archived Ministral 8B results
```

## Infrastructure

- **Local (no GPU):** Data processing, classic ML, visualization via Jupyter notebooks
- **Modal (cloud GPU):** All LLM inference and fine-tuning (L4 for inference, A100 for training)
- **External APIs:** Mistral Batch API, DeepInfra API (for DeepSeek V3.2 and GLM-5)
- **Total compute cost:** ~$54

## Reproduction

1. Place raw ASRS CSV files in `raw data/` directory
2. Run `notebooks/01_data_exploration.ipynb` to generate `data/asrs_multilabel.csv`
3. Run `notebooks/02_experiment_pipeline.ipynb` to create train/test splits and classic ML baseline
4. Run Modal scripts (`scripts/modal_*.py`) for LLM experiments (requires Modal account)
5. Run API scripts (`scripts/mistral_large_*.py`, `scripts/deepseek_v32_deepinfra.py`) for API-based experiments
6. Run `notebooks/03_final_visualizations.ipynb` to generate all figures

## License

This repository is part of a university thesis submission. All code is provided for academic review.
